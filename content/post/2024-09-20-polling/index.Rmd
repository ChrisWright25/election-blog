---
title: Polling
author: Chris Wright
date: '2024-09-20'
slug: polling
categories: []
tags: []
---





```{r message=FALSE, warning=FALSE, include=FALSE}
## Packages
library(car)
library(caret)
library(CVXR)
library(glmnet)
library(tidyverse)
library(ggpubr)

## set working directory here
setwd("~/Gov1347/election-blog/content/post/2024-09-20-polling")

#Read in files
polls_2016 <- read_csv("president_polls_2016.csv")
polls_2020 <- read_csv("president_polls_2020.csv")
polls_2024 <- read_csv("president_polls_2024.csv")

d_vote <- read_csv("popvote_1948-2020.csv")
d_vote$party[d_vote$party == "democrat"] <- "DEM"
d_vote$party[d_vote$party == "republican"] <- "REP"

# Subset for US Presidential race
polls_2016 <- subset(polls_2016, polls_2016$state == "U.S.")

# Renaming variables / cleanup for merging
all_polls <- merge(polls_2016, polls_2020, by = "poll_id", all = TRUE)

all_polls$new_grade <- ifelse(is.na(all_polls$grade), all_polls$fte_grade, all_polls$grade)

all_polls$dem_polling <- ifelse(is.na(all_polls$adjpoll_clinton), all_polls$pct, all_polls$adjpoll_clinton)

all_polls$party <- ifelse(is.na(all_polls$party), "DEM", all_polls$party)

all_polls$dem_pv <- ifelse(is.na(all_polls$cycle.x), 51.31571, 47.05731)

# Subset based on pollster quality
polls_A <- subset(all_polls, new_grade == "A+" | new_grade == "A"| new_grade == "A-")
polls_B <- subset(all_polls, new_grade == "B+" | new_grade == "B"| new_grade == "B-")
polls_C <- subset(all_polls, new_grade == "C+" | new_grade == "C"| new_grade == "C-")

# Set up for prediction
polls_2024$dem_polling <- polls_2024$pct
polls_2024_dems <- subset(polls_2024, polls_2024$party == "DEM")

# Predicting using A+, A, and A- polls
polls_A_dems <- subset(polls_A, party == "DEM")
A_polls_lm <- lm(polls_A_dems$dem_pv ~ polls_A_dems$dem_polling)
summary(A_polls_lm)
A_predict <- mean(predict(A_polls_lm, polls_2024_dems))

# Predicting using B+, B, and B- polls
polls_B_dems <- subset(polls_B, party == "DEM")
B_polls_lm <- lm(polls_B_dems$dem_pv ~ polls_B_dems$dem_polling)
summary(B_polls_lm)
B_predict <- mean(predict(B_polls_lm, polls_2024_dems))

# Predicting using C+, C, and C- polls
polls_C_dems <- subset(polls_C, party == "DEM")
C_polls_lm <- lm(polls_C_dems$dem_pv ~ polls_C_dems$dem_polling)
summary(C_polls_lm)
C_predict <- mean(predict(C_polls_lm, polls_2024_dems))


#Ensemble prediction
combine_predict <- (A_predict + B_predict + C_predict) / 3


# Further understanding the quality of polls

summary(polls_A_dems$dem_polling)
summary(polls_B_dems$dem_polling)
summary(polls_C_dems$dem_polling)

summary(polls_A_dems$sample_size)
summary(polls_B_dems$sample_size)
summary(polls_C_dems$sample_size)


```


```{r echo=FALSE, fig.align='center', out.height="33%", out.width="33%"}
# Adding in image of Gallup
knitr::include_graphics("gallup.jpg")

```

```{r fig.align='center', message=FALSE, warning=FALSE, include=FALSE, out.height="33%", out.width="33%"}

# Adding in image of Gallup
knitr::include_graphics("gallup.jpg")

# Formatting data for prediction chart (Grade - A)
# Adding days left column
polls_A_dems$days_left <- as.numeric(as.Date(polls_A_dems$forecastdate, "%m/%d/%Y") - as.Date(polls_A_dems$createddate, "%m/%d/%Y"))

# Grouping the data by days left
polls_A_dems_grouped = polls_A_dems %>% group_by(days_left)  %>%
    summarise(aggregate_prediction = mean(dem_polling))

# Plotting prediciton over time
grade_A_plot <- ggplot() + geom_line(aes(y = aggregate_prediction, x = days_left), data = polls_A_dems_grouped) + labs(x = "Days left", y = "Democrat Prediction", title = "A Rated Polls") + theme(plot.title = element_text(hjust = 0.5)) + geom_hline(yintercept=49.18651, linetype = "dashed")


# Formatting data for prediction chart (Grade - B)
# Adding days left column
polls_B_dems$days_left <- as.numeric(as.Date(polls_B_dems$forecastdate, "%m/%d/%Y") - as.Date(polls_B_dems$createddate, "%m/%d/%Y"))

# Grouping the data by days left
polls_B_dems_grouped = polls_B_dems %>% group_by(days_left)  %>%
    summarise(aggregate_prediction = mean(dem_polling))

# Plotting prediction over time
grade_B_plot <- ggplot() + geom_line(aes(y = aggregate_prediction, x = days_left), data = polls_B_dems_grouped) + labs(x = "Days left", y = "Democrat Prediction", title = "B Rated Polls") + theme(plot.title = element_text(hjust = 0.5)) + geom_hline(yintercept=49.18651, linetype = "dashed")


# Formatting data for prediction chart (Grade - C)
# Adding days left column
polls_C_dems$days_left <- as.numeric(as.Date(polls_C_dems$forecastdate, "%m/%d/%Y") - as.Date(polls_C_dems$createddate, "%m/%d/%Y"))

# Grouping the data by days left
polls_C_dems_grouped = polls_C_dems %>% group_by(days_left)  %>%
    summarise(aggregate_prediction = mean(dem_polling))

# Plotting prediction over time
grade_C_plot <- ggplot() + geom_line(aes(y = aggregate_prediction, x = days_left), data = polls_C_dems_grouped) + labs(x = "Days left", y = "Democrat Prediction", title = "C Rated Polls") + theme(plot.title = element_text(hjust = 0.5)) + geom_hline(yintercept=49.18651, linetype = "dashed")

combined_plot <- ggarrange(grade_A_plot, grade_B_plot, grade_C_plot, ncol = 1)
```



The picture above is of George Gallup. Gallup is considered to be the father of public opinion polling and founded the namesake polling organization, Gallup. Since Gallup revolutionized polling, it has been a mainstay when talking about elections. Although polls are widely conducted and a focal point of campaign news, the validity and accuracy of polls has always been in question. In the 1948 election, polls famously predicted a Dewey victory over Truman and more recently some polls underestimated the support for Trump in the 2016 election. These polling mishaps lead to one essential question: What makes a good poll?


## 538 Pollster Ratings

538 attempts to answer the question by giving each poll that it includes in its aggregation a [rating](https://projects.fivethirtyeight.com/pollster-ratings/). In the past this rating was on a scale from A+ to D-, but now the rating is on a scale from 0 to 3. According to 538's [website](https://fivethirtyeight.com/methodology/how-our-pollster-ratings-work/) the rating takes into account several factors

* Simple error for polls (i.e., how far away the poll results are from the actual election margin).
* How well other pollsters performed in the same races (i.e., whether this pollster is as good as, better than or worse than others).
* Methodological quality (i.e., whether this pollster is conducting polls in accordance with professional standards).
* Herding (i.e., whether this pollster appears to just be copying othersâ€™ results).

## Evaluating the Ratings

Utilizing polls from the 2016 and 2020 election cycle (before the numeric pollster ratings) I analyzed the difference in polling ratings. For the analysis, I grouped the pollster ratings into three categories: A (A+, A, and A-), B (B+, B, and B-), and C (C+, C, and C-).

The first observation from this analysis is the large number of C rated polls compared to A and B rated polls. There are almost 1000 more polls in the C range than the A and B range. C polls also stand out on sample size. The mean sample size of C polls is 4x the mean of A polls. C polls also seem to have a more Democratic tilt, as they predict Democratic support 4 points higher than A and B polls on average. Lastly, C polls appear more prone to outliers as they have both a poll with a sample size of 88 and one with a 94% prediction.

Pollster Rating | Number of Polls | Mean Sample Size | Max Sample Size | Min Sample Size | Mean DEM prediction | Max Dem Prediction | Min Dem Prediction
------------- | ------------- | ------------- | ------------- | ------------- | ------------- | ------------- | -------------
A | 2783 | 1122 | 5459 | 301 | 46.45 | 69.44 | 34.00
B | 2323 | 2722 | 71789 | 201 | 46.12 | 64.00 | 20.00
C | 3643 | 5043 | 42858 | 88 | 50.39 | 94.00 | 21.48

Additionally, in an attempt to see accuracy over time, I plotted the average poll prediction for Democratic candidates. The dotted line represents the actual election outcome. Although the C polls have more range, they seem to get closer to the actual prediction as the election date is closer

Note: The dotted line is a sum of the 2016 and 2020 election since the dataset covers the 2016 and 2020 elections


```{r echo=FALSE}
combined_plot
```



## Predicting Based on the Ratings

Lastly, I used the A, B, and C polls to predict the 2024 election to see if there is any difference in the predictions. Before fitting the model, I subset it to only predictions for the Democratic party. Then I fit the model using the projected Democratic vote share and the actual election outcome in either 2016 or 2020. Lastly, I used 2024 polling data to predict based on the model. In future analysis, I would include older polls to have more data points; however, I was unable to find older polling data with the associated 538 pollster ratings.

Model | Predicted DEM Vote | R-squared
------------- | ------------- | -------------
A | 49.15% | 0.2853
B | 50.65% | 0.0004535
C | 50.53% | 0.06187

Ultimately, all of the models had pretty low R-squared values which means that they explain little of the variance in the data. However, the A prediction model had the best R-squared. The models also gave similar predictions and the B model gave the most support to Democrats. When all the models are combined, they predict that Democrats will capture __50.1%__ of the vote in the 2024 election. According to my analysis, there is little variation in the prediction based on pollster ratings. However, the higher R-squared leads me to believe that A polls are better than lower rated ones.













